<?xml version="1.0" ?>
<doc>
	<label auto="true" type="str" verify="true"><![CDATA[Develop]]></label>
	<author auto="true" type="list" verify="true">
		<item type="str"><![CDATA[dobry-kot]]></item>
	</author>
	<date auto="true" type="str" verify="true"><![CDATA[2022-12-08, 22:35]]></date>
	<link auto="true" type="str" verify="true"><![CDATA[https://habr.com/ru/post/704410/]]></link>
	<title auto="true" type="str" verify="true"><![CDATA[Kubernetes the hard way (Эволюция Часть 1)]]></title>
	<categories auto="true" type="list" verify="true">
		<item type="str"><![CDATA[*nix]]></item>
		<item type="str"><![CDATA[Kubernetes]]></item>
	</categories>
	<key_words auto="true" type="list" verify="true">
		<item type="str"><![CDATA[kubernetes]]></item>
		<item type="str"><![CDATA[linux]]></item>
	</key_words>
	<text auto="true" type="str" verify="true"><![CDATA[Всем привет. Меня зовут Добрый Кот Telegram.
В этой статье расскажем, как развернуть кластер в Yandex Cloud нашим модулем terraform.
От коллектива FR-Solutions и при поддержке @irbgeo Telegram : Продолжаем серию статей о K8S.
Состояние на момент написания статьи
Освежим воспоминания
В предыдущей статье мы рассписали процес развертывания Kubernetes с использованием голых конфигов и базовых бинарных файлов - это рабочий вариант, но в наших реалиях этого недостаточно, мы хотим автоматизацию.
* Пока писали автоматизацию, испытали море эмоций как от специфики шаблонизации, так и наследования ресурсов, но об это позже.
Отрицание
Первая автоматизация с моей стороны была написана на Ansible. Все начиналось лампово, пара тасок там, пара тут и через месяц родилась первая версия IaC кубера на Ansible, но чем больше я писал на Ansible, тем больше понимал, что его недостаточно - нужно постоянно удалять и создавать окружения в облаке.
Для создания окружения в облаке прекрасно подходит terraform, и первая мысль заключалась в том, чтобы скрестить ужа с ежом и получить первые 5 метров колючей проволки.
Получилось весьма неплохо, но в ходе эксплуатации я понял, что от Ansible можно избавиться полностью и написать все на Terraform. Связано это было с тем, что самое важное для сборки kubernetes моим методом - требуется всего лишь один правильный Cloud-INIT и вот для него расчехлять Ansible не хотелось. Ну что, сказано - сделано! Пошли писать на Terraform.
Гнев
Больше всего нервов скушал модуль для Vault, именно его кодовая база является самой большой в нашей автоматизации. Самые трудоемкие части:
Спецификация для будущих сертификатов. В ней содержится вся информация:
Описание для ролей.
Описание для заказа Key-keeper.
Описание сейфов и их CA.
Мета применимости.
Описание политик.
Описание токенов и аппролей.
В попытке решить базовую проблему безопасности - как передать в ВМ мастера временный токен через Cloud-Init - мы сотворили монстра. (На момент написания статьи Yandex Cloud наконец выпустил terraform provider для внутреннего сервиса LockBox, который позволяет решить проблему через IAM) - скоро пофиксим.
Торг
Дальше пытались задизайнить максимально простую, но достаточно высокодоступную конфигурацию и пришли к вот такой схеме.
Концепт простой, сделать так, чтобы виртуальные машины стали Stateless и не хранили никакой полезной нагрузки, и чтобы, в случае чего, их можно было без труда пересобрать (даже на другой тип операционной системы).
Чтобы такое провернуть, требутся несколько моментов:
База Etcd должна быть вынесена на внешние диски.
Сертификаты должны заезжать на ноду сами (сертификаты заказывает key-keeper).
Весь конфиг должен быть описан в Cloud-init.
У ВМ должны быть статичные A записи.
Мы решили все эти задачи и теперь можем работать с узлами мастеров достаточно гибко.
Депрессия
Первая версия на терраформе выглядела как одна большая репка
с ресурсами из-за чего часто были проблемы с добавлением нового или дебага старого. Было принято решение распилить на модули все ресурсы, что вызывало много вопросов "а как лучше?".
В основе лежит первый модуль yandex-cluster, в нем описывается спецификация будущего кластера и кастомизация входных аргументов модуля.
init
Для инициализации Control Plane достаточно создать VPC и подсети для мастеров, а в Vault должен быть создан корневой сертификат компании.
Дальше кастомные параметры попадают в модуль k8s-config-vars - в этом модуле формируются и растекаются по всем остальным модулям все переменные окружения кластера.
После того, как переменные окружения сформированы, мы собираем для каждой будущей ноды Cloud-init окружение в Vault. От k8s-vault-master мы получаем токен и подкидываем в Coud-init.
Последним действием мы создаем нужную инфраструктуру и узлы с нашими Cloud-Init конфигами.
Принятие
Как итог работы мы получили рабочий модуль для Yandex Cloud по созданию Control Plane Kubernetes. Когда я дописывал последние строки, я прям почувствовал себя Риком Огурчиком!
Эмоций это приключение породило очень много и не все позитивные. Самой большой проблемой было использование модулей внутри модуля, использование наследования переменных и шаблонизация самого терраформа. В перспективе, надеюсь, упростим конфигурацию.
Что дальше?
В принципе, уже на данный момент кластер можно собрать по примеру этого флоу
Вы получите:
Контрол Плейн K8S в HA режиме
Развернутый ворклоуд Cilium, CoreDNS, YandexCSIDriver, YandexCloudController, Gatekeeper, Certmanager, ClusterMachineApprover
В кластер добавится Worker узел - с сертификатами от волта, но выписанный кубом (Немного магии - расскажем позже)
Перед собой ставим цели собрать минимальную коробку с бестпрактисами:
Дописать модуль для Data-Plane (заказ workers).
Добавить интеграцию с etcd-backup-restore и yandex s3.
Настроить базовый мониторинг.
Настроить SSO на базе Keycloak.
Добавить Автоскейлер.
Переписать на GO рендер cloud-init.
Отрефакторить RBAC для мастеров.
Переделать мастера c instances на group-instances
Вишенка на торте
Статус
Ждем вас на обсуждения нашей работы в https://t.me/fraima_ru
Полезное чтиво
Kubernetes The Hard Way
https://github.com/fraima/kubernetes
https://github.com/fraima/key-keeper Всем привет. Меня зовут Добрый Кот Telegram. Telegram В этой статье расскажем, как развернуть кластер в Yandex Cloud нашим модулем terraform. От коллектива FR-Solutions и при поддержке @irbgeo Telegram : Продолжаем серию статей о K8S. FR-Solutions @irbgeo Telegram Состояние на момент написания статьи  Состояние на момент написания статьи Освежим воспоминания В предыдущей статье мы рассписали процес развертывания Kubernetes с использованием голых конфигов и базовых бинарных файлов - это рабочий вариант, но в наших реалиях этого недостаточно, мы хотим автоматизацию. В предыдущей статье * Пока писали автоматизацию, испытали море эмоций как от специфики шаблонизации, так и наследования ресурсов, но об это позже. Отрицание Первая автоматизация с моей стороны была написана на Ansible. Все начиналось лампово, пара тасок там, пара тут и через месяц родилась первая версия IaC кубера на Ansible, но чем больше я писал на Ansible, тем больше понимал, что его недостаточно - нужно постоянно удалять и создавать окружения в облаке. Для создания окружения в облаке прекрасно подходит terraform, и первая мысль заключалась в том, чтобы скрестить ужа с ежом и получить первые 5 метров колючей проволки. Получилось весьма неплохо, но в ходе эксплуатации я понял, что от Ansible можно избавиться полностью и написать все на Terraform. Связано это было с тем, что самое важное для сборки kubernetes моим методом - требуется всего лишь один правильный Cloud-INIT и вот для него расчехлять Ansible не хотелось. Ну что, сказано - сделано! Пошли писать на Terraform. Гнев Больше всего нервов скушал модуль для Vault, именно его кодовая база является самой большой в нашей автоматизации. Самые трудоемкие части: Спецификация для будущих сертификатов. В ней содержится вся информация:
Описание для ролей.
Описание для заказа Key-keeper.
Описание сейфов и их CA.
Мета применимости.
Описание политик.
Описание токенов и аппролей. Спецификация для будущих сертификатов. В ней содержится вся информация:
Описание для ролей.
Описание для заказа Key-keeper.
Описание сейфов и их CA.
Мета применимости. Спецификация для будущих сертификатов. В ней содержится вся информация: Спецификация Описание для ролей.
Описание для заказа Key-keeper.
Описание сейфов и их CA.
Мета применимости. Описание для ролей. Описание для ролей. Описание для заказа Key-keeper. Описание для заказа Key-keeper. Описание сейфов и их CA. Описание сейфов и их CA. Мета применимости. Мета применимости. Описание политик. Описание политик. Описание токенов и аппролей. Описание токенов и аппролей. В попытке решить базовую проблему безопасности - как передать в ВМ мастера временный токен через Cloud-Init - мы сотворили монстра. (На момент написания статьи Yandex Cloud наконец выпустил terraform provider для внутреннего сервиса LockBox, который позволяет решить проблему через IAM) - скоро пофиксим. Торг Дальше пытались задизайнить максимально простую, но достаточно высокодоступную конфигурацию и пришли к вот такой схеме.    Концепт простой, сделать так, чтобы виртуальные машины стали Stateless и не хранили никакой полезной нагрузки, и чтобы, в случае чего, их можно было без труда пересобрать (даже на другой тип операционной системы). Чтобы такое провернуть, требутся несколько моментов: База Etcd должна быть вынесена на внешние диски.
Сертификаты должны заезжать на ноду сами (сертификаты заказывает key-keeper).
Весь конфиг должен быть описан в Cloud-init.
У ВМ должны быть статичные A записи. База Etcd должна быть вынесена на внешние диски. База Etcd должна быть вынесена на внешние диски. Сертификаты должны заезжать на ноду сами (сертификаты заказывает key-keeper). Сертификаты должны заезжать на ноду сами (сертификаты заказывает key-keeper). Весь конфиг должен быть описан в Cloud-init. Весь конфиг должен быть описан в Cloud-init. У ВМ должны быть статичные A записи. У ВМ должны быть статичные A записи. Мы решили все эти задачи и теперь можем работать с узлами мастеров достаточно гибко. Депрессия Первая версия на терраформе выглядела как одна большая репка    с ресурсами из-за чего часто были проблемы с добавлением нового или дебага старого. Было принято решение распилить на модули все ресурсы, что вызывало много вопросов "а как лучше?".   В основе лежит первый модуль yandex-cluster, в нем описывается спецификация будущего кластера и кастомизация входных аргументов модуля. init init       Для инициализации Control Plane достаточно создать VPC и подсети для мастеров, а в Vault должен быть создан корневой сертификат компании. Дальше кастомные параметры попадают в модуль k8s-config-vars - в этом модуле формируются и растекаются по всем остальным модулям все переменные окружения кластера. k8s-config-vars - После того, как переменные окружения сформированы, мы собираем для каждой будущей ноды Cloud-init окружение в Vault. От k8s-vault-master мы получаем токен и подкидываем в Coud-init. Cloud-init Vault k8s-vault-master Coud-init. Последним действием мы создаем нужную инфраструктуру и узлы с нашими Cloud-Init конфигами. Cloud-Init Принятие   Как итог работы мы получили рабочий модуль для Yandex Cloud по созданию Control Plane Kubernetes. Когда я дописывал последние строки, я прям почувствовал себя Риком Огурчиком! Yandex Cloud Эмоций это приключение породило очень много и не все позитивные. Самой большой проблемой было использование модулей внутри модуля, использование наследования переменных и шаблонизация самого терраформа. В перспективе, надеюсь, упростим конфигурацию. Что дальше? В принципе, уже на данный момент кластер можно собрать по примеру этого флоу этого флоу Вы получите: Контрол Плейн K8S в HA режиме
Развернутый ворклоуд Cilium, CoreDNS, YandexCSIDriver, YandexCloudController, Gatekeeper, Certmanager, ClusterMachineApprover
В кластер добавится Worker узел - с сертификатами от волта, но выписанный кубом (Немного магии - расскажем позже) Контрол Плейн K8S в HA режиме Контрол Плейн K8S в HA режиме Развернутый ворклоуд Cilium, CoreDNS, YandexCSIDriver, YandexCloudController, Gatekeeper, Certmanager, ClusterMachineApprover Развернутый ворклоуд Cilium, CoreDNS, YandexCSIDriver, YandexCloudController, Gatekeeper, Certmanager, ClusterMachineApprover В кластер добавится Worker узел - с сертификатами от волта, но выписанный кубом (Немного магии - расскажем позже) В кластер добавится Worker узел - с сертификатами от волта, но выписанный кубом (Немного магии - расскажем позже) Перед собой ставим цели собрать минимальную коробку с бестпрактисами: Дописать модуль для Data-Plane (заказ workers).
Добавить интеграцию с etcd-backup-restore и yandex s3.
Настроить базовый мониторинг.
Настроить SSO на базе Keycloak.
Добавить Автоскейлер.
Переписать на GO рендер cloud-init.
Отрефакторить RBAC для мастеров.
Переделать мастера c instances на group-instances Дописать модуль для Data-Plane (заказ workers). Дописать модуль для Data-Plane (заказ workers). Добавить интеграцию с etcd-backup-restore и yandex s3. Добавить интеграцию с etcd-backup-restore и yandex s3. Настроить базовый мониторинг. Настроить базовый мониторинг. Настроить SSO на базе Keycloak. Настроить SSO на базе Keycloak. Добавить Автоскейлер. Добавить Автоскейлер. Переписать на GO рендер cloud-init. Переписать на GO рендер cloud-init. Отрефакторить RBAC для мастеров. Отрефакторить RBAC для мастеров. Переделать мастера c instances на group-instances Переделать мастера c instances на group-instances Вишенка на торте Статус Статус          Ждем вас на обсуждения нашей работы в https://t.me/fraima_ru https://t.me/fraima_ru Полезное чтиво Kubernetes The Hard Way
https://github.com/fraima/kubernetes
https://github.com/fraima/key-keeper Kubernetes The Hard Way  https://github.com/fraima/kubernetes  https://github.com/fraima/key-keeper ]]></text>
</doc>
