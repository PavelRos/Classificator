<?xml version="1.0" ?>
<doc>
	<label auto="true" type="str" verify="true"><![CDATA[Other]]></label>
	<author auto="true" type="list" verify="true">
		<item type="str"><![CDATA[tairsu]]></item>
	</author>
	<date auto="true" type="str" verify="true"><![CDATA[2022-10-16, 11:35]]></date>
	<link auto="true" type="str" verify="true"><![CDATA[https://habr.com/ru/post/693580/]]></link>
	<title auto="true" type="str" verify="true"><![CDATA[Как нарисовать барашка или прикладное нейрохудожество]]></title>
	<categories auto="true" type="list" verify="true">
		<item type="str"><![CDATA[Работа с видео]]></item>
		<item type="str"><![CDATA[Разработка игр]]></item>
		<item type="str"><![CDATA[Обработка изображений]]></item>
		<item type="str"><![CDATA[Компьютерная анимация]]></item>
	</categories>
	<key_words auto="true" type="list" verify="true">
		<item type="str"><![CDATA[ai]]></item>
		<item type="str"><![CDATA[image]]></item>
		<item type="str"><![CDATA[deep learning]]></item>
		<item type="str"><![CDATA[stablediffusion]]></item>
		<item type="str"><![CDATA[anime]]></item>
	</key_words>
	<text auto="true" type="str" verify="true"><![CDATA[Часть 1. Первые шаги
Потихоньку начнём разговор для понимания приёмов и возможностей нейросетевых инструментов. Они с нами навсегда, неплохо бы разобраться в самом начале.
Для примера выберем, конечно же, девчонок.
На картинке результат работы нейросети от компании StableDiffusion полученный на локальной машине человеком, который до 2022 года не смог бы нарисовать сову по инструкции. А теперь он способен на такой портрет.
Правда у него появилась волшебная кисточка - нейросеть (минимальные требования к кисточке относительно низки: видеокарта 1050 4Gb+, 16Gb RAM и около 20Gb места на диске). Как же стать таким талантом? Очень просто, вы скачали и установили Python, поставили обработчик, запустили интерфейс - и вот, вы художник!

Поговорим сразу о некоторых настройках.
Вот внизу вы видите матрицу довольно фривольного вида изображений, в качестве исходника для которых, был взят мем из сети Интернет. Текст запроса:
Prompth: full frontal, stunning blonde, (perfect face), suggestive, bright smile, gorgeous hair, squat,
Negative prompt: paint, anime, cartoon, art,drawing,3d render, digital painting, body out of frame, ((deformed)), (cross–eyed), (closed eyes), blurry, (bad anatomy), ugly, disfigured, ((poorly drawn face)), (mutation), (mutated), (extra limbs), (bad body)
Перед началом генерирования у вас есть много разных настроек и возможностей . Например запрос может быть отрицательным - Negative prompt. Это фактически запрет, очень мощный инструмент, в котором описывается чего бы вы не хотели видеть в результирующем изображении. Как правило туда включают все заклинания призванные исключить появление в результирующем изображении шестипалых рук и множества других мутаций, которые только отнимают ценное время. Рисовать правильные пальцы - это вообще больное место нейросетей, поэтому их часто и не видно в сгенерированных изображениях. Впрочем, продолжим.
В самом сейчас популярном интерфейсе к нейросети Stable Diffusion есть возможность одним нажатием запустить процесс генерирования изображений в цикле с переменными которые подставляются по мере генерирования изображений. Таким образом можно получить результат в виде матрицы где есть все заданные вами варианты. Например на приведенной иллюстрации в левом верхнем углу можно увидеть фактически первоначальное изображение, а вниз и вправо идут постепенные изменения. В данном случае запушен запрос для генерации пакета(batch) сразу из 6 случайных вариаций (seed) и каждый пакет еще и с переменными шагами и деформацией.
Шаги (steps)— это количество проходов нейросетью для достижения результата.
Деформация (denoise) — это степень влияния исходного изображения на процесс. Чтобы разобраться в том, как вообще устроен процесс надо быть очень погруженным в тему, но если кратко — то процесс создания изображения начинается с заполнения пространства изображения случайным шумом (noise) (кстати исходное число для генератора случайности называют вариацией (seed).Об этом чуть подробнее в 3 части.
Соответственно слева направо идет увеличение количества проходов (шагов, steps), а сверху вниз — увеличивается параметр деформации, то есть нейросеть все меньше(!) внимания обращает на исходное изображение. При 1, например, исходное изображение вообще игнорируется и изображение рисуется заново, из шума.
Как видите, в правом нижнем углу, там где на исходник уже не обращают внимания, наблюдается полное разнообразие (напомню, запушен пакет (batch) из 6 вариаций(seed).
И тут мы переходим к роли Sampler (обработчика), о котором я расскажу в следующий раз.
Часть 2. Sampler
Теперь поговорим о Sampler, можно перевести как Обработчик, но скорее всего все будут использовать максимум, пару из всех возможных поэтому достаточно будет просто заимствовать слово сэмплер.
Сначала немного о сути происходящего. Все картинки нейросеть воссоздает по вашему текстовому запросу (prompt) путем распознавания деталей и отброса лишнего в существующем изображении. Вы можете к текстовому запросу загрузить первоначальное изображение — тогда включается инструмент img2img. Если изображения у вас нет, а есть только текстовый запрос "сделай мне красиво", то запустится txt2img.
По сути они представляют собой одно и тоже, просто когда у нейросети на входе только ваш текстовый запрос и больше ничего, она первым делом включает txt2img,а уж потом img2img. Задача первого - создать заданного размера прямоугольник и заполнить его случайным цветным шумом (случайность которого обеспечивает генератор случайных чисел выдающий нейросети некое случайное число — seed). А вот если изображение вы загрузили сразу своё, то первый этап не нужен и включается img2img. Ваше изображение для него служит шумом, при этом есть настройка (denoise) насколько оно должно влиять на результат, про это было в первой части.
img2img это и есть, собственно, таинство нейрорисования.
Путем нескольких проходов (steps) по каждому пикселю изображения нейросеть пытается распознать во входном шуме, именно те объекты и их взаимоотношения (например, отражение в зеркале или падающий свет из окна) которые вы ей задали в запросе (prompt). Как показывает накапливающийся с каждым днём опыт — лучше первоначальное изображение все таки иметь. Подойдет даже детский набросок в Paint, в котором вы примерно обрисуете на что рассчитываете. Без изображения, прямо с буквального нуля нейросети потребуется больше попыток, а вам соответственно времени, чтобы угадать какой именно из бесконечного множества вариантов наиболее подходит по композиции.
Каждый сэмплер распознает по своему алгоритму, пользуясь огромной базой в несколько гигабайт (model.ckpt), в которой находится результат обработки миллионов изображений в сжатом виде (вот бы кто–то взялся и написал, как можно весь изобразительный опыт человечества ужать в 4Gb, это же чудо какое–то).
Сэмплеры хороши по разному, но самый популярный (и не зря он первый по списку, это Euler A.
А теперь — слайд. Сверху вниз разные сэплеры, слева направо — количество проходов. Из слайда видно, что большинство сэмплеров на первых шагах выбирают какое–то изображение и улучшают его с каждым шагом. А Euler A каждый раз придумывает что–то новое.
Вдумчивое видео на эту тему (английский).
До новых встреч, в следующий раз поговорим о главном — о моделях.
Часть 3. Модели
Итак, о моделях. О натренированных, современных моделях для изобразительных нейросетей.
В заглавии статьи вы все видели эффектный результат простого запроса:
A portrait, busty girl, cleavage, looking at camera, choker, rainbow hair
Если его исполнить на стандартной, доступной на октябрь 2022, обученной модели Stable Diffusion 1.4 (sd–v1–4.ckpt, около 4Gb), то полученное изображение по этому запросу будет выглядеть совершенно иначе. Почему? Потому что этот запрос исполнялся на другой модели — WaifuDiffusion 1.3.
Чтобы объяснить, что именно это значит, давайте разберемся в том, что такое модель применительно к нейросетям изображений. Это, ни много ни мало, модель вселенной, порожденная нейросетью, которую долго кормили изображениями. Каждое изображение при этом имело текстовое описание того, что на нем изображено.
Миллионы доступных в Интернет изображений, фотографий, рисунков были перемолоты в жерновах тысяч мощных видеокарт,для того чтобы на выходе получился один файл, который и описывает взаимоотношения отдельных пикселей на основе текстового описания. Этот файл называют моделью. В целом, это очень интересная тема, но ML не входит в цель данного поста, поэтому гуглите сами, если интересно.
Как образованные (испорченные) интернетом люди могут понять из названия WaifuDiffusion, (для простоты WD) эта модель имеет отношение к anime. И правда. Она натренирована на сотнях тысяч изображений популярного в узких кругах anime форума Danbooru. И ёе, можно сказать, основное предназначение, это генерация anime всех возможных стилей и изображений.
Специализированные модели очень хороши в том, для чего они предназначены, но хромают во всем остальном, например WD не сможет сделать нормальную фотографию — она почти не знает, как они выглядят. Специализированных именно на anime моделей сейчас очень много, это какой–то огромный мне неведомый пласт жизни азиатского (и не только) интернета, где живут pony, furry, pokemon, единороги и много других для кого–то очень привлекательных слов. Там даже бывают свои драмы, например пару дней назад спиратили модель, которая использовалась для платного (я не шучу) сервиса генерации anime. NovelAI. Теперь она бесплатно доступна энтузиастам.
Люди с хорошими видеокартами создают свои модели, скармливая нейросети только те изображения, которые им интересны. В частности самый популярный сейчас метод, Textual Inversion, накормить сеть своими собственными фотографиями. После этого вы сможете добавлять себя в запросы и появляться в результирующих изображениях. Можно прямо в интерфейсе комбинировать две разные модели, или скачать готовую модель, где уже совмещены какие–то наиболее популярные, как правило в них указывается, какая модель какой вес имеет в результате. Внутри моделей, есть еще натренированные вставки, embeddings, это когда люди добавляют к существующей модели кусок данных, натренированных самостоятельно, но я пока в этом еще не разбирался.
Самый большой выбор моделей (возбуждение — великая сила энтузиазма) относится, конечно, к порноиндустрии. С этого года, надо отметить будет некоторый сдвиг в борьбе с ЦП, потому что не совсем ясно, как остановить людей, которые на своих видеокартах будут на дому создавать то, что раньше приходилось искать в темных закоулках под страхом закона. И как определить возраст согласия сгенерированной мадмуазели, тоже пока неясно. То же самое относится к deepfake, никто же не мешает накормить домашнюю нейросеть фотографиями знакомых людей. Мы еще даже не представляем, что люди способны выпечь на отложенных в ящики сотнях видеокарт оставшихся после конца майнинга.
Кстати, популярные фамилии и имена уже сейчас определенно влияют на результат запросов, поэтому мой совет, дорогие ценители женской нейрокрасоты, указывайте в запросах название популярной женщины. Впрочем, мы отвлеклись.
Обычно при публикации изображений, максимум чем люди делятся - это текст запроса, мало кто указывает на какой модели было сгенерировано то или иное изображение. Но эта информация всегда есть где- то сохраняется, вот пример из описания
Steps: 30, Sampler: Euler a, CFG scale: 8, Seed: 1139688548, Size: 896x640, Model hash: 7460a6fa, Model: sd–v1–4
В конце две переменных hash — это какой файл модели был использован и model — примерно тоже самое но на человеческом языке. Таким образом, чтобы получить достоверную информацию о способе создания того или иного изображения, недостаточно запроса, нужно знать еще и на какой модели оно генерировалось. Но для создания интересных изображений, художник прорабатывает по отдельности, поэтому даже зная всё, повторить результат можно только приближенно.

Постоянно растущий список моделей для скачивания, вообще на этом сайте всё полезное. Переключать модели в самом популярном интерфейсе от Automatic1111, sd webui, можно прямо сверху, на первой же странице.
На этом пока всё, в следующий раз поговорим об инструменте inpaint вписыванию нужного фрагмента в имеющееся изображение. До встречи, успехов вам и хорошего охлаждения вашей видеокарте! Часть 1. Первые шаги Потихоньку начнём разговор для понимания приёмов и возможностей нейросетевых инструментов. Они с нами навсегда, неплохо бы разобраться в самом начале.
Для примера выберем, конечно же, девчонок.    На картинке результат работы нейросети от компании StableDiffusion полученный на локальной машине человеком, который до 2022 года не смог бы нарисовать сову по инструкции. А теперь он способен на такой портрет. Правда у него появилась волшебная кисточка - нейросеть (минимальные требования к кисточке относительно низки: видеокарта 1050 4Gb+, 16Gb RAM и около 20Gb места на диске). Как же стать таким талантом? Очень просто, вы скачали и установили Python, поставили обработчик, запустили интерфейс - и вот, вы художник!

Поговорим сразу о некоторых настройках. Очень просто   Вот внизу вы видите матрицу довольно фривольного вида изображений, в качестве исходника для которых, был взят мем из сети Интернет. Текст запроса: Prompth: full frontal, stunning blonde, (perfect face), suggestive, bright smile, gorgeous hair, squat,
Negative prompt: paint, anime, cartoon, art,drawing,3d render, digital painting, body out of frame, ((deformed)), (cross–eyed), (closed eyes), blurry, (bad anatomy), ugly, disfigured, ((poorly drawn face)), (mutation), (mutated), (extra limbs), (bad body)  Перед началом генерирования у вас есть много разных настроек и возможностей . Например запрос может быть отрицательным - Negative prompt. Это фактически запрет, очень мощный инструмент, в котором описывается чего бы вы не хотели видеть в результирующем изображении. Как правило туда включают все заклинания призванные исключить появление в результирующем изображении шестипалых рук и множества других мутаций, которые только отнимают ценное время. Рисовать правильные пальцы - это вообще больное место нейросетей, поэтому их часто и не видно в сгенерированных изображениях. Впрочем, продолжим. В самом сейчас популярном интерфейсе к нейросети Stable Diffusion есть возможность одним нажатием запустить процесс генерирования изображений в цикле с переменными которые подставляются по мере генерирования изображений. Таким образом можно получить результат в виде матрицы где есть все заданные вами варианты. Например на приведенной иллюстрации в левом верхнем углу можно увидеть фактически первоначальное изображение, а вниз и вправо идут постепенные изменения. В данном случае запушен запрос для генерации пакета(batch) сразу из 6 случайных вариаций (seed) и каждый пакет еще и с переменными шагами и деформацией. Шаги (steps)— это количество проходов нейросетью для достижения результата. Деформация (denoise) — это степень влияния исходного изображения на процесс. Чтобы разобраться в том, как вообще устроен процесс надо быть очень погруженным в тему, но если кратко — то процесс создания изображения начинается с заполнения пространства изображения случайным шумом (noise) (кстати исходное число для генератора случайности называют вариацией (seed).Об этом чуть подробнее в 3 части. Соответственно слева направо идет увеличение количества проходов (шагов, steps), а сверху вниз — увеличивается параметр деформации, то есть нейросеть все меньше(!) внимания обращает на исходное изображение. При 1, например, исходное изображение вообще игнорируется и изображение рисуется заново, из шума. Как видите, в правом нижнем углу, там где на исходник уже не обращают внимания, наблюдается полное разнообразие (напомню, запушен пакет (batch) из 6 вариаций(seed). И тут мы переходим к роли Sampler (обработчика), о котором я расскажу в следующий раз.    Часть 2. Sampler Теперь поговорим о Sampler, можно перевести как Обработчик, но скорее всего все будут использовать максимум, пару из всех возможных поэтому достаточно будет просто заимствовать слово сэмплер. Сначала немного о сути происходящего. Все картинки нейросеть воссоздает по вашему текстовому запросу (prompt) путем распознавания деталей и отброса лишнего в существующем изображении. Вы можете к текстовому запросу загрузить первоначальное изображение — тогда включается инструмент img2img. Если изображения у вас нет, а есть только текстовый запрос "сделай мне красиво", то запустится txt2img. img2img txt2img По сути они представляют собой одно и тоже, просто когда у нейросети на входе только ваш текстовый запрос и больше ничего, она первым делом включает txt2img,а уж потом img2img. Задача первого - создать заданного размера прямоугольник и заполнить его случайным цветным шумом (случайность которого обеспечивает генератор случайных чисел выдающий нейросети некое случайное число — seed). А вот если изображение вы загрузили сразу своё, то первый этап не нужен и включается img2img. Ваше изображение для него служит шумом, при этом есть настройка (denoise) насколько оно должно влиять на результат, про это было в первой части. txt2img img2img. seed img2img img2img это и есть, собственно, таинство нейрорисования. img2img Путем нескольких проходов (steps) по каждому пикселю изображения нейросеть пытается распознать во входном шуме, именно те объекты и их взаимоотношения (например, отражение в зеркале или падающий свет из окна) которые вы ей задали в запросе (prompt). Как показывает накапливающийся с каждым днём опыт — лучше первоначальное изображение все таки иметь. Подойдет даже детский набросок в Paint, в котором вы примерно обрисуете на что рассчитываете. Без изображения, прямо с буквального нуля нейросети потребуется больше попыток, а вам соответственно времени, чтобы угадать какой именно из бесконечного множества вариантов наиболее подходит по композиции.
Каждый сэмплер распознает по своему алгоритму, пользуясь огромной базой в несколько гигабайт (model.ckpt), в которой находится результат обработки миллионов изображений в сжатом виде (вот бы кто–то взялся и написал, как можно весь изобразительный опыт человечества ужать в 4Gb, это же чудо какое–то). steps prompt  model.ckpt Сэмплеры хороши по разному, но самый популярный (и не зря он первый по списку, это Euler A. Euler A А теперь — слайд. Сверху вниз разные сэплеры, слева направо — количество проходов. Из слайда видно, что большинство сэмплеров на первых шагах выбирают какое–то изображение и улучшают его с каждым шагом. А Euler A каждый раз придумывает что–то новое. Euler A Вдумчивое видео на эту тему (английский). Вдумчивое видео До новых встреч, в следующий раз поговорим о главном — о моделях.    Часть 3. Модели Итак, о моделях. О натренированных, современных моделях для изобразительных нейросетей. В заглавии статьи вы все видели эффектный результат простого запроса:
A portrait, busty girl, cleavage, looking at camera, choker, rainbow hair  A portrait, busty girl, cleavage, looking at camera, choker, rainbow hair Если его исполнить на стандартной, доступной на октябрь 2022, обученной модели Stable Diffusion 1.4 (sd–v1–4.ckpt, около 4Gb), то полученное изображение по этому запросу будет выглядеть совершенно иначе. Почему? Потому что этот запрос исполнялся на другой модели — WaifuDiffusion 1.3. Stable Diffusion 1.4 sd–v1–4.ckpt WaifuDiffusion 1.3 Чтобы объяснить, что именно это значит, давайте разберемся в том, что такое модель применительно к нейросетям изображений. Это, ни много ни мало, модель вселенной, порожденная нейросетью, которую долго кормили изображениями. Каждое изображение при этом имело текстовое описание того, что на нем изображено. Миллионы доступных в Интернет изображений, фотографий, рисунков были перемолоты в жерновах тысяч мощных видеокарт,для того чтобы на выходе получился один файл, который и описывает взаимоотношения отдельных пикселей на основе текстового описания. Этот файл называют моделью. В целом, это очень интересная тема, но ML не входит в цель данного поста, поэтому гуглите сами, если интересно. гуглите сами Как образованные (испорченные) интернетом люди могут понять из названия WaifuDiffusion, (для простоты WD) эта модель имеет отношение к anime. И правда. Она натренирована на сотнях тысяч изображений популярного в узких кругах anime форума Danbooru. И ёе, можно сказать, основное предназначение, это генерация anime всех возможных стилей и изображений. Специализированные модели очень хороши в том, для чего они предназначены, но хромают во всем остальном, например WD не сможет сделать нормальную фотографию — она почти не знает, как они выглядят. Специализированных именно на anime моделей сейчас очень много, это какой–то огромный мне неведомый пласт жизни азиатского (и не только) интернета, где живут pony, furry, pokemon, единороги и много других для кого–то очень привлекательных слов. Там даже бывают свои драмы, например пару дней назад спиратили модель, которая использовалась для платного (я не шучу) сервиса генерации anime. NovelAI. Теперь она бесплатно доступна энтузиастам. pony, furry, pokemon, единороги Люди с хорошими видеокартами создают свои модели, скармливая нейросети только те изображения, которые им интересны. В частности самый популярный сейчас метод, Textual Inversion, накормить сеть своими собственными фотографиями. После этого вы сможете добавлять себя в запросы и появляться в результирующих изображениях. Можно прямо в интерфейсе комбинировать две разные модели, или скачать готовую модель, где уже совмещены какие–то наиболее популярные, как правило в них указывается, какая модель какой вес имеет в результате. Внутри моделей, есть еще натренированные вставки, embeddings, это когда люди добавляют к существующей модели кусок данных, натренированных самостоятельно, но я пока в этом еще не разбирался. Textual Inversion embeddings Самый большой выбор моделей (возбуждение — великая сила энтузиазма) относится, конечно, к порноиндустрии. С этого года, надо отметить будет некоторый сдвиг в борьбе с ЦП, потому что не совсем ясно, как остановить людей, которые на своих видеокартах будут на дому создавать то, что раньше приходилось искать в темных закоулках под страхом закона. И как определить возраст согласия сгенерированной мадмуазели, тоже пока неясно. То же самое относится к deepfake, никто же не мешает накормить домашнюю нейросеть фотографиями знакомых людей. Мы еще даже не представляем, что люди способны выпечь на отложенных в ящики сотнях видеокарт оставшихся после конца майнинга. Кстати, популярные фамилии и имена уже сейчас определенно влияют на результат запросов, поэтому мой совет, дорогие ценители женской нейрокрасоты, указывайте в запросах название популярной женщины. Впрочем, мы отвлеклись. Обычно при публикации изображений, максимум чем люди делятся - это текст запроса, мало кто указывает на какой модели было сгенерировано то или иное изображение. Но эта информация всегда есть где- то сохраняется, вот пример из описания Steps: 30, Sampler: Euler a, CFG scale: 8, Seed: 1139688548, Size: 896x640, Model hash: 7460a6fa, Model: sd–v1–4 Steps: 30, Sampler: Euler a, CFG scale: 8, Seed: 1139688548, Size: 896x640, Model hash: 7460a6fa, Model: sd–v1–4 В конце две переменных hash — это какой файл модели был использован и model — примерно тоже самое но на человеческом языке. Таким образом, чтобы получить достоверную информацию о способе создания того или иного изображения, недостаточно запроса, нужно знать еще и на какой модели оно генерировалось. Но для создания интересных изображений, художник прорабатывает по отдельности, поэтому даже зная всё, повторить результат можно только приближенно.

Постоянно растущий список моделей для скачивания, вообще на этом сайте всё полезное. Переключать модели в самом популярном интерфейсе от Automatic1111, sd webui, можно прямо сверху, на первой же странице. hash model   Постоянно растущий список моделей На этом пока всё, в следующий раз поговорим об инструменте inpaint вписыванию нужного фрагмента в имеющееся изображение. До встречи, успехов вам и хорошего охлаждения вашей видеокарте! ]]></text>
</doc>
