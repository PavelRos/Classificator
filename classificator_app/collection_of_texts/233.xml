<?xml version="1.0" ?>
<doc>
	<label auto="true" type="str" verify="true"><![CDATA[Develop]]></label>
	<author auto="true" type="list" verify="true">
		<item type="str"><![CDATA[matantsev]]></item>
	</author>
	<date auto="true" type="str" verify="true"><![CDATA[2022-12-11, 02:26]]></date>
	<link auto="true" type="str" verify="true"><![CDATA[https://habr.com/ru/post/704710/]]></link>
	<title auto="true" type="str" verify="true"><![CDATA[Self-Supervised Learning. Проблематика и постановка задачи]]></title>
	<categories auto="true" type="list" verify="true">
		<item type="str"><![CDATA[Машинное обучение]]></item>
		<item type="str"><![CDATA[Искусственный интеллект]]></item>
	</categories>
	<key_words auto="true" type="list" verify="true">
		<item type="str"><![CDATA[computer vision]]></item>
		<item type="str"><![CDATA[neural]]></item>
		<item type="str"><![CDATA[imagenet]]></item>
		<item type="str"><![CDATA[self-supervised]]></item>
	</key_words>
	<text auto="true" type="str" verify="true"><![CDATA[В последнее 2-3 года в обучении нейросеток всё больше набирает обороты self-supervised подход. Это мощный инструмент, который позволяет использовать огромные массивы данных, при этом не требуя трудозатратной разметки. Так можно учиться на миллионах или даже миллиардах картинок. Благодаря такому подходу были получены большинство state-of-the-art результатов в последнее время на классических датасетах типа ImageNet.
Это первая статья в цикле, которая рассматривает основные преимущества Self-Supervised Learning (SSL) и общую постановку задачи. Цикл будет посвящён SSL для Computer Vision.
Некоторые проблемы обучения без SSL
Рандомная инициализация моделей
Классический подход к обучению нейросеток подразумевает случайную инициализацию весов модели. Поскольку параметров в современных моделях миллионы (а то и миллиарды), какого-то более разумного принципа инициализации чем рандомом придумать сложно. При этом, если в экспериментах пробуются разные архитектуры, или тюнятся параметры, или как-то меняется датасет, или что ещё - то происходит, опять же, рандомная реинициализация весов и погнали снова двигать шум в сторону нанесения пользы.
Малый размер обучающих выборок
Это, конечно, зависит от задачи, но часто очень дорого и сложно собрать датасеты с качественной разметкой. Как примеры можно смело привести медицинские данные или спутниковые снимки. Фактически размеченные датасеты редко покрывают всё разнообразие таких снимков. Следовательно, от малых объёмов страдают финальные метрики модели на реальных данных.
Ресурсоёмкость обучения
Прямое следствие из первого пункта. При обучении каждый раз с рандома затрачивается большое количество ресурсов на сходимость модели к чему-то осознанному. Следовательно, старт с какого-то хорошего чекпойнта может заметно ускорить сходимость по конкретной задаче.
Претрен как решение
Как можно заметить, многие проблемы порождены 1-ым пунктом - рандомной инициализацией. Следовательно, избежав рандома на старте, можно отчасти их решить.
Одно из решений - делать претрен на датасетах, которые больше, чем датасет по нашей задаче. Как пример, мы хотим распознавать модель ноутбука по фотографии, у нас есть размеченный датасет на 50к картинок. Для получения хорошего чекпойнта модели сделаем претрен - скачиваем размеченный ImageNet на 1М картинок и обучаемся на нём. По результату мы имеем модель, которая умеет доставать фичи из картинок и делает это достаточно неплохо. Вместо рандомной инициализации используем этот чекпойнт, дообучаем его на нашу задачу. Исторически этот метод хорошо себя зарекомендовал, многие используют.
Однако, сохраняется проблема малой выборки. Если наша исходная выборка 50к мала по сравнению с 1М, то 1М картинок - малая выборка по сравнению со всеми картинками в интернете (сложно дать оценку количеству). Для улучшения качества нам необходимо как-то извлекать сигнал и из них тоже.
Ещё одним минусом такого подхода может быть различие доменов, к которым относятся изображения. ImageNet - набор изображений 1000 категорий предметов или животных. Эти фотографии структурно отличаются от тех же спутниковых снимков, поэтому фиче экстрактор, который мы претренили на ImageNet'e, может быть не оптимальным для поиска лесов из космоса. Эта задача называется Domain adaptation и она не является решенной.
SSL как решение
В предыдущем параграфе мы использовали датасет с большим количеством размеченных данных для создания хорошего чекпойнта модели. Иначе говоря, мы сформулировали прокси-задачу (Pretext task) - обучить модель на ImageNet - задачу, которая отличается от нашей, но структурно близкая. При этом оригинальная задача (Downstream task) имела supervised формулировку - натренировать классификатор ноутбуков - разметка на неё была дана.
Если мы хотим обучаться на миллионах неразмеченных данных, нам также надо сформулировать прокси-задачу таким образом, чтобы нам не нужна была изначальная разметка. Собственно, формулировка таких прокси задач и есть основное направление поиска в исследованиях SSL.
Таким подходом можно решить проблему Domain Adaptation, при наличии большого неразмеченного датасета в требуемом домене.
SSL - так что это такое?
Мы подбирались к теме со стороны проблематики, давайте теперь дадим какое-то более чёткое определение.
Self-Supervised Learning (или unsupervised) - режим обучения моделей, при котором задача обучения не требует дополнительной разметки и формируется исходя из внутренней структуры самих объектов, либо из базовых знаний об объектах. Преимущество таких методов состоит как раз в использовании информации, которые мы уже знаем про каждое конкретное изображение, следовательно нет необходимости дополнительно делать разметку.
Методы SSL начали массированно появляться в Computer Vision после успехов в NLP задачах. Так, например, одной из причин популярности и успешности BERT является как раз SSL. Типичная задача при обучении BERTа - маскировать слово в предложении и заставлять модель восстановить его из контекста. Использование таких SSL трюков докидывало от 1 до 6 процентов accuracy в зависимости от датасета.
Примерами подобных задач для картинок выступают, например, воссоздание правильного порядка последовательности перемешанных кусков одного изображения. Или колоризация изображения, которое исходно было цветное, а на вход в сетку подаётся его чб версия. Более подробно различные методы будут рассмотрены далее.
Существуют также методы, которые позволяют взять лучшее из 2-х миров - использовать одновременно как размеченные, так и не размеченные данные. Такие подходы называются Semi-Supervised, здесь мы не будем их касаться.
Систематизация подходов
Немного устаканим терминологию и классификацию методов. Итак, pretext task - задача SSL, которую мы формулируем для предобучения модели. Downstream task - задача, которую мы хотим непосредственно решить. По сути, решая pretext task мы обучаем модель, или backbone, или feature extractor, чтобы потом переиспользовать его в downstream task. Backbone при этом преобразует изображение в embedding - вектор в некотором пространстве признаков.
Формулировки методов SSL можно разделить на 3 группы:
Генеративные модели
Методы, основанные на внутренней семантике изображений
Joint Embedding Methods
Генеративные модели мы почти не будем обсуждать, достаточно будет заметить, что в GAN-подобных сетях дискриминатор на вход получает изображение, генерирует эмбеддинг, для постановки задачи не нужны метки классов. Следовательно обучение дискриминатора можно рассматривать как SSL задачу, но результаты получаются не очень.
Методы, основанные на внутренней семантике изображений, мы рассмотрим во второй статье, это наиболее старые методы, сейчас так уже (почти) не делают.
Joint Embedding Methods - методы, позволяющие обучать модель на основе выводов о совместном расположени представлений (embeddings) различных или аугментированных версий изображений. Наиболее широкий класс формулировок, к нему принадлежат все SOTA методы за последние 4 года. Им будет посвящено аж 3 статьи.
Вместо итога
Мы достаточно широкими мазками рассмотрели проблематику SSL и основные понятия в этой теме. Я рассмтриваю эту статью как некую рамку, на которой можно выстраивать повествование. В последующих статьях мы подробнее затронем основные методы, которые используются на практике и историю их развития.
Если вы ещё не вдохновились полезностью SSL подходов, то приглашаю почитать последнюю статью цикла, там больше конкретики про полезность. Ну и вот вам последняя затравочка: Ян Ле Кун в твиттере признался, что видит SSL одним из величайших открытий в ML за последнее десятилетие.
Список статей в цикле
SSL. Проблематика и постановка задачи
SSL. Метрики и первые pretext tasks
SSL. Обучение на изображении и его аугментациях
SSL. Contrastive learning
SSL. Кластеризация как лосс
SSL. Результаты и основные фреймворки
Дополнительные ссылки
Self-supervised learning: The dark matter of intelligence at Meta Blog
Цикл лекций Яна ЛеКуна в NYU. Вот тут конспект лекций.
Либо одна лекция Яна ЛеКуна Rise of Self-Supervised Learning
Self-Supervised Learning. Simple and intuitive introduction for beginners
A Survey on Contrastive Self-Supervised Learning
Подробный обзор методов SSL
Ещё один подробный обзор методов SSL
Обзор методов SSL для Multimodal and Temporal Data
Awesome Self-Supervised Learning
Отличный тред в твиттере В последнее 2-3 года в обучении нейросеток всё больше набирает обороты self-supervised подход. Это мощный инструмент, который позволяет использовать огромные массивы данных, при этом не требуя трудозатратной разметки. Так можно учиться на миллионах или даже миллиардах картинок. Благодаря такому подходу были получены большинство state-of-the-art результатов в последнее время на классических датасетах типа ImageNet. Это первая статья в цикле, которая рассматривает основные преимущества Self-Supervised Learning (SSL) и общую постановку задачи. Цикл будет посвящён SSL для Computer Vision. Некоторые проблемы обучения без SSL Рандомная инициализация моделей
Классический подход к обучению нейросеток подразумевает случайную инициализацию весов модели. Поскольку параметров в современных моделях миллионы (а то и миллиарды), какого-то более разумного принципа инициализации чем рандомом придумать сложно. При этом, если в экспериментах пробуются разные архитектуры, или тюнятся параметры, или как-то меняется датасет, или что ещё - то происходит, опять же, рандомная реинициализация весов и погнали снова двигать шум в сторону нанесения пользы.
Малый размер обучающих выборок
Это, конечно, зависит от задачи, но часто очень дорого и сложно собрать датасеты с качественной разметкой. Как примеры можно смело привести медицинские данные или спутниковые снимки. Фактически размеченные датасеты редко покрывают всё разнообразие таких снимков. Следовательно, от малых объёмов страдают финальные метрики модели на реальных данных.
Ресурсоёмкость обучения
Прямое следствие из первого пункта. При обучении каждый раз с рандома затрачивается большое количество ресурсов на сходимость модели к чему-то осознанному. Следовательно, старт с какого-то хорошего чекпойнта может заметно ускорить сходимость по конкретной задаче. Рандомная инициализация моделей
Классический подход к обучению нейросеток подразумевает случайную инициализацию весов модели. Поскольку параметров в современных моделях миллионы (а то и миллиарды), какого-то более разумного принципа инициализации чем рандомом придумать сложно. При этом, если в экспериментах пробуются разные архитектуры, или тюнятся параметры, или как-то меняется датасет, или что ещё - то происходит, опять же, рандомная реинициализация весов и погнали снова двигать шум в сторону нанесения пользы. Рандомная инициализация моделей
Классический подход к обучению нейросеток подразумевает случайную инициализацию весов модели. Поскольку параметров в современных моделях миллионы (а то и миллиарды), какого-то более разумного принципа инициализации чем рандомом придумать сложно. При этом, если в экспериментах пробуются разные архитектуры, или тюнятся параметры, или как-то меняется датасет, или что ещё - то происходит, опять же, рандомная реинициализация весов и погнали снова двигать шум в сторону нанесения пользы.  Малый размер обучающих выборок
Это, конечно, зависит от задачи, но часто очень дорого и сложно собрать датасеты с качественной разметкой. Как примеры можно смело привести медицинские данные или спутниковые снимки. Фактически размеченные датасеты редко покрывают всё разнообразие таких снимков. Следовательно, от малых объёмов страдают финальные метрики модели на реальных данных. Малый размер обучающих выборок
Это, конечно, зависит от задачи, но часто очень дорого и сложно собрать датасеты с качественной разметкой. Как примеры можно смело привести медицинские данные или спутниковые снимки. Фактически размеченные датасеты редко покрывают всё разнообразие таких снимков. Следовательно, от малых объёмов страдают финальные метрики модели на реальных данных.  Ресурсоёмкость обучения
Прямое следствие из первого пункта. При обучении каждый раз с рандома затрачивается большое количество ресурсов на сходимость модели к чему-то осознанному. Следовательно, старт с какого-то хорошего чекпойнта может заметно ускорить сходимость по конкретной задаче. Ресурсоёмкость обучения
Прямое следствие из первого пункта. При обучении каждый раз с рандома затрачивается большое количество ресурсов на сходимость модели к чему-то осознанному. Следовательно, старт с какого-то хорошего чекпойнта может заметно ускорить сходимость по конкретной задаче.  Претрен как решение Как можно заметить, многие проблемы порождены 1-ым пунктом - рандомной инициализацией. Следовательно, избежав рандома на старте, можно отчасти их решить.
Одно из решений - делать претрен на датасетах, которые больше, чем датасет по нашей задаче. Как пример, мы хотим распознавать модель ноутбука по фотографии, у нас есть размеченный датасет на 50к картинок. Для получения хорошего чекпойнта модели сделаем претрен - скачиваем размеченный ImageNet на 1М картинок и обучаемся на нём. По результату мы имеем модель, которая умеет доставать фичи из картинок и делает это достаточно неплохо. Вместо рандомной инициализации используем этот чекпойнт, дообучаем его на нашу задачу. Исторически этот метод хорошо себя зарекомендовал, многие используют.  Однако, сохраняется проблема малой выборки. Если наша исходная выборка 50к мала по сравнению с 1М, то 1М картинок - малая выборка по сравнению со всеми картинками в интернете (сложно дать оценку количеству). Для улучшения качества нам необходимо как-то извлекать сигнал и из них тоже.
Ещё одним минусом такого подхода может быть различие доменов, к которым относятся изображения. ImageNet - набор изображений 1000 категорий предметов или животных. Эти фотографии структурно отличаются от тех же спутниковых снимков, поэтому фиче экстрактор, который мы претренили на ImageNet'e, может быть не оптимальным для поиска лесов из космоса. Эта задача называется Domain adaptation и она не является решенной.  SSL как решение В предыдущем параграфе мы использовали датасет с большим количеством размеченных данных для создания хорошего чекпойнта модели. Иначе говоря, мы сформулировали прокси-задачу (Pretext task) - обучить модель на ImageNet - задачу, которая отличается от нашей, но структурно близкая. При этом оригинальная задача (Downstream task) имела supervised формулировку - натренировать классификатор ноутбуков - разметка на неё была дана. Если мы хотим обучаться на миллионах неразмеченных данных, нам также надо сформулировать прокси-задачу таким образом, чтобы нам не нужна была изначальная разметка. Собственно, формулировка таких прокси задач и есть основное направление поиска в исследованиях SSL. Таким подходом можно решить проблему Domain Adaptation, при наличии большого неразмеченного датасета в требуемом домене. SSL - так что это такое? Мы подбирались к теме со стороны проблематики, давайте теперь дадим какое-то более чёткое определение.
Self-Supervised Learning (или unsupervised) - режим обучения моделей, при котором задача обучения не требует дополнительной разметки и формируется исходя из внутренней структуры самих объектов, либо из базовых знаний об объектах. Преимущество таких методов состоит как раз в использовании информации, которые мы уже знаем про каждое конкретное изображение, следовательно нет необходимости дополнительно делать разметку.  Self-Supervised Learning уже Методы SSL начали массированно появляться в Computer Vision после успехов в NLP задачах. Так, например, одной из причин популярности и успешности BERT является как раз SSL. Типичная задача при обучении BERTа - маскировать слово в предложении и заставлять модель восстановить его из контекста. Использование таких SSL трюков докидывало от 1 до 6 процентов accuracy в зависимости от датасета. BERT Примерами подобных задач для картинок выступают, например, воссоздание правильного порядка последовательности перемешанных кусков одного изображения. Или колоризация изображения, которое исходно было цветное, а на вход в сетку подаётся его чб версия. Более подробно различные методы будут рассмотрены далее. Существуют также методы, которые позволяют взять лучшее из 2-х миров - использовать одновременно как размеченные, так и не размеченные данные. Такие подходы называются Semi-Supervised, здесь мы не будем их касаться. Систематизация подходов Немного устаканим терминологию и классификацию методов. Итак, pretext task - задача SSL, которую мы формулируем для предобучения модели. Downstream task - задача, которую мы хотим непосредственно решить. По сути, решая pretext task мы обучаем модель, или backbone, или feature extractor, чтобы потом переиспользовать его в downstream task. Backbone при этом преобразует изображение в embedding - вектор в некотором пространстве признаков. Формулировки методов SSL можно разделить на 3 группы: Генеративные модели
Методы, основанные на внутренней семантике изображений
Joint Embedding Methods Генеративные модели Генеративные модели Методы, основанные на внутренней семантике изображений Методы, основанные на внутренней семантике изображений Joint Embedding Methods Joint Embedding Methods Генеративные модели мы почти не будем обсуждать, достаточно будет заметить, что в GAN-подобных сетях дискриминатор на вход получает изображение, генерирует эмбеддинг, для постановки задачи не нужны метки классов. Следовательно обучение дискриминатора можно рассматривать как SSL задачу, но результаты получаются не очень. Методы, основанные на внутренней семантике изображений, мы рассмотрим во второй статье, это наиболее старые методы, сейчас так уже (почти) не делают. Joint Embedding Methods - методы, позволяющие обучать модель на основе выводов о совместном расположени представлений (embeddings) различных или аугментированных версий изображений. Наиболее широкий класс формулировок, к нему принадлежат все SOTA методы за последние 4 года. Им будет посвящено аж 3 статьи. Вместо итога Мы достаточно широкими мазками рассмотрели проблематику SSL и основные понятия в этой теме. Я рассмтриваю эту статью как некую рамку, на которой можно выстраивать повествование. В последующих статьях мы подробнее затронем основные методы, которые используются на практике и историю их развития. Если вы ещё не вдохновились полезностью SSL подходов, то приглашаю почитать последнюю статью цикла, там больше конкретики про полезность. Ну и вот вам последняя затравочка: Ян Ле Кун в твиттере признался, что видит SSL одним из величайших открытий в ML за последнее десятилетие. признался Список статей в цикле SSL. Проблематика и постановка задачи
SSL. Метрики и первые pretext tasks
SSL. Обучение на изображении и его аугментациях
SSL. Contrastive learning
SSL. Кластеризация как лосс
SSL. Результаты и основные фреймворки SSL. Проблематика и постановка задачи SSL. Проблематика и постановка задачи SSL. Проблематика и постановка задачи SSL. Метрики и первые pretext tasks SSL. Метрики и первые pretext tasks SSL. Метрики и первые pretext tasks SSL. Обучение на изображении и его аугментациях SSL. Обучение на изображении и его аугментациях SSL. Обучение на изображении и его аугментациях SSL. Contrastive learning SSL. Contrastive learning SSL. Contrastive learning SSL. Кластеризация как лосс SSL. Кластеризация как лосс SSL. Кластеризация как лосс SSL. Результаты и основные фреймворки SSL. Результаты и основные фреймворки SSL. Результаты и основные фреймворки Дополнительные ссылки Self-supervised learning: The dark matter of intelligence at Meta Blog
Цикл лекций Яна ЛеКуна в NYU. Вот тут конспект лекций.
Либо одна лекция Яна ЛеКуна Rise of Self-Supervised Learning
Self-Supervised Learning. Simple and intuitive introduction for beginners
A Survey on Contrastive Self-Supervised Learning
Подробный обзор методов SSL
Ещё один подробный обзор методов SSL
Обзор методов SSL для Multimodal and Temporal Data
Awesome Self-Supervised Learning
Отличный тред в твиттере Self-supervised learning: The dark matter of intelligence at Meta Blog Self-supervised learning: The dark matter of intelligence at Meta Blog Self-supervised learning: The dark matter of intelligence at Meta Blog Цикл лекций Яна ЛеКуна в NYU. Вот тут конспект лекций. Цикл лекций Яна ЛеКуна в NYU. Вот тут конспект лекций. Цикл лекций Яна ЛеКуна в NYU тут Либо одна лекция Яна ЛеКуна Rise of Self-Supervised Learning Либо одна лекция Яна ЛеКуна Rise of Self-Supervised Learning Rise of Self-Supervised Learning Self-Supervised Learning. Simple and intuitive introduction for beginners Self-Supervised Learning. Simple and intuitive introduction for beginners Self-Supervised Learning. Simple and intuitive introduction for beginners A Survey on Contrastive Self-Supervised Learning A Survey on Contrastive Self-Supervised Learning A Survey on Contrastive Self-Supervised Learning Подробный обзор методов SSL Подробный обзор методов SSL Подробный обзор методов SSL Ещё один подробный обзор методов SSL Ещё один подробный обзор методов SSL Ещё один подробный обзор методов SSL Обзор методов SSL для Multimodal and Temporal Data Обзор методов SSL для Multimodal and Temporal Data Обзор методов SSL для Multimodal and Temporal Data Awesome Self-Supervised Learning Awesome Self-Supervised Learning Awesome Self-Supervised Learning Отличный тред в твиттере Отличный тред в твиттере Отличный тред в твиттере ]]></text>
</doc>
